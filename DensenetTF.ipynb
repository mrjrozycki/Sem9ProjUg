{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-24 11:20:56.099158: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-24 11:20:56.192278: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/jrozycki/miniconda3/envs/DensenetUG/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2023-01-24 11:20:56.192290: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-01-24 11:20:56.636353: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/jrozycki/miniconda3/envs/DensenetUG/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2023-01-24 11:20:56.636397: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/jrozycki/miniconda3/envs/DensenetUG/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2023-01-24 11:20:56.636401: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer(nn.Sequential):\n",
    "    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate):\n",
    "        super(DenseLayer, self).__init__()\n",
    "        self.add_module('norm1', nn.BatchNorm2d(num_input_features)),\n",
    "        self.add_module('relu1', nn.ReLU(inplace=True)),\n",
    "        self.add_module('conv1', nn.Conv2d(num_input_features, bn_size *\n",
    "                        growth_rate, kernel_size=1, stride=1, bias=False)),\n",
    "        self.add_module('norm2', nn.BatchNorm2d(bn_size * growth_rate)),\n",
    "        self.add_module('relu2', nn.ReLU(inplace=True)),\n",
    "        self.add_module('conv2', nn.Conv2d(bn_size * growth_rate, growth_rate,\n",
    "                        kernel_size=3, stride=1, padding=1, bias=False)),\n",
    "        self.drop_rate = drop_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        new_features = super(DenseLayer, self).forward(x)\n",
    "        if self.drop_rate > 0:\n",
    "            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)\n",
    "        return torch.cat([x, new_features], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transision(nn.Sequential):\n",
    "    def __init__(self, num_input_features, num_output_features):\n",
    "        super(Transision, self).__init__()\n",
    "        self.add_module('norm', nn.BatchNorm2d(num_input_features)),\n",
    "        self.add_module('relu', nn.ReLU(inplace=True)),\n",
    "        self.add_module('conv', nn.Conv2d(num_input_features, num_output_features,\n",
    "                        kernel_size=1, stride=1, bias=False)),\n",
    "        self.add_module('pool', nn.AvgPool2d(kernel_size=2, stride=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseBlock(nn.Sequential):\n",
    "    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate):\n",
    "        super(DenseBlock, self).__init__()\n",
    "        for i in range(num_layers):\n",
    "            layer = DenseLayer(num_input_features + i * growth_rate, growth_rate, bn_size, drop_rate)\n",
    "            self.add_module('denselayer%d' % (i + 1), layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, growth_rate=32, block_config=(6, 12, 24, 16), num_init_features=64, bn_size=4, drop_rate=0, num_classes=1):\n",
    "        super(DenseNet, self).__init__()\n",
    "        self.features = nn.Sequential(OrderedDict([\n",
    "            ('conv0', nn.Conv2d(3, num_init_features, kernel_size=7, stride=2, padding=3, bias=False)),\n",
    "            ('norm0', nn.BatchNorm2d(num_init_features)),\n",
    "            ('relu0', nn.ReLU(inplace=True)),\n",
    "            ('pool0', nn.MaxPool2d(kernel_size=3, stride=2, padding=1)),\n",
    "        ]))\n",
    "        num_features = num_init_features\n",
    "        for i, num_layers in enumerate(block_config):\n",
    "            block = DenseBlock(num_layers=num_layers, num_input_features=num_features, bn_size=bn_size, growth_rate=growth_rate, drop_rate=drop_rate)\n",
    "            self.features.add_module('denseblock%d' % (i + 1), block)\n",
    "            num_features = num_features + num_layers * growth_rate\n",
    "            if i != len(block_config) - 1:\n",
    "                trans = Transision(num_input_features=num_features, num_output_features=num_features // 2)\n",
    "                self.features.add_module('transition%d' % (i + 1), trans)\n",
    "                num_features = num_features // 2\n",
    "\n",
    "        self.features.add_module('norm5', nn.BatchNorm2d(num_features))\n",
    "        self.classifier = nn.Linear(num_features, num_classes)\n",
    "\n",
    "        # Official init from torch repo.\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal(m.weight.data)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        out = F.relu(features, inplace=True)\n",
    "        out = F.adaptive_avg_pool2d(out, (1, 1)).view(features.size(0), -1)\n",
    "        out = torch.sigmoid(self.classifier(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def densenet121(pretrained=False, **kwargs):\n",
    "    model = DenseNet(num_init_features=64, growth_rate=32, block_config=(6, 12, 24, 16), **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['densenet121']))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jrozycki/miniconda3/envs/DensenetUG/lib/python3.10/site-packages/deeplake/util/check_latest_version.py:32: UserWarning: A newer version of deeplake (3.2.1) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import copy\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from train import train_model, get_metrics\n",
    "from pipeline import get_study_level_data, get_dataloaders\n",
    "from DenseNetData import GetDatasetSize\n",
    "import deeplake\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Cancer': 465, 'normal': 202} \n",
      "\n",
      " {'Cancer': 59, 'normal': 13} \n",
      "\n",
      " {'Cancer': 261, 'normal': 54}\n"
     ]
    }
   ],
   "source": [
    "train_path = \"./Data/train\"\n",
    "val_path = \"./Data/valid\"\n",
    "test_path = \"./Data/test\"\n",
    "    \n",
    "train_set = GetDatasetSize(train_path)\n",
    "val_set = GetDatasetSize(val_path)\n",
    "test_set = GetDatasetSize(test_path)\n",
    "print(train_set,\"\\n\\n\",val_set,\"\\n\\n\",test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Cancer', 'Normal']\n",
    "train_list = list(train_set.values())\n",
    "val_list = list(val_set.values())\n",
    "test_list = list(test_set.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tai = {'train': train_list[0], 'valid': val_list[0], 'test': test_list[0]}\n",
    "tni = {'train': train_list[1], 'valid': val_list[1], 'test': test_list[1]}\n",
    "data_cat = ['train', 'valid', 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_p(x):\n",
    "    '''convert numpy float to Variable tensor float'''    \n",
    "    return Variable(torch.FloatTensor([x]), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wt1 = {x: n_p(tni[x] / (tni[x] + tai[x])) for x in data_cat}\n",
    "Wt0 = {x: n_p(tai[x] / (tni[x] + tai[x])) for x in data_cat}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tai: {'train': 465, 'valid': 59, 'test': 261}\n",
      "tni: {'train': 202, 'valid': 13, 'test': 54} \n",
      "\n",
      "Wt0 train: tensor([0.6972])\n",
      "Wt0 valid: tensor([0.8194])\n",
      "Wt1 train: tensor([0.3028])\n",
      "Wt1 valid: tensor([0.1806])\n"
     ]
    }
   ],
   "source": [
    "print('tai:', tai)\n",
    "print('tni:', tni, '\\n')\n",
    "print('Wt0 train:', Wt0['train'])\n",
    "print('Wt0 valid:', Wt0['valid'])\n",
    "print('Wt1 train:', Wt1['train'])\n",
    "print('Wt1 valid:', Wt1['valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00,  9.41it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 91.48it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 20.76it/s]\n"
     ]
    }
   ],
   "source": [
    "study_data = get_study_level_data(\"normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 667, 'valid': 72, 'test': 315}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(study_data)\n",
    "\n",
    "dataloaders = get_dataloaders(study_data)\n",
    "dataset_sizes = {x: len(study_data[x]) for x in data_cat}\n",
    "dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(torch.nn.modules.Module):\n",
    "    def __init__(self, Wt1, Wt0):\n",
    "        super(Loss, self).__init__()\n",
    "        self.Wt1 = Wt1\n",
    "        self.Wt0 = Wt0\n",
    "        \n",
    "    def forward(self, output, target, data_cat):\n",
    "        loss = self.Wt1[data_cat] * (target * torch.log(output)) + self.Wt0[data_cat] * ((1 - target) * torch.log(1 - output))\n",
    "        return -torch.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_72124/3600136440.py:26: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  nn.init.kaiming_normal(m.weight.data)\n"
     ]
    }
   ],
   "source": [
    "model = densenet121(pretrained=False)\n",
    "criteria = Loss(Wt1, Wt0)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 334\n",
      "Valid batches: 36 \n",
      "\n",
      "Epoch 1/3\n",
      "----------\n",
      "train Loss: 0.1407 Acc: 0.5892\n",
      "Confusion Meter:\n",
      " [[0.6089109  0.3910891 ]\n",
      " [0.41935483 0.58064514]]\n",
      "valid Loss: 0.2782 Acc: 0.8194\n",
      "Confusion Meter:\n",
      " [[0. 1.]\n",
      " [0. 1.]]\n",
      "Time elapsed: 1m 54s\n",
      "\n",
      "Epoch 2/3\n",
      "----------\n",
      "train Loss: 0.1170 Acc: 0.7241\n",
      "Confusion Meter:\n",
      " [[0.8366337  0.16336633]\n",
      " [0.32473117 0.6752688 ]]\n",
      "valid Loss: 0.1522 Acc: 0.1111\n",
      "Confusion Meter:\n",
      " [[0.15384616 0.84615386]\n",
      " [0.89830506 0.10169491]]\n",
      "Time elapsed: 3m 49s\n",
      "\n",
      "Epoch 3/3\n",
      "----------\n",
      "train Loss: 0.0998 Acc: 0.7436\n",
      "Confusion Meter:\n",
      " [[0.9108911  0.08910891]\n",
      " [0.32903227 0.67096776]]\n",
      "valid Loss: nan Acc: 0.8194\n",
      "Confusion Meter:\n",
      " [[0. 1.]\n",
      " [0. 1.]]\n",
      "Time elapsed: 5m 48s\n",
      "\n",
      "Training complete in 5m 48s\n",
      "Best valid Acc: 0.819444\n"
     ]
    }
   ],
   "source": [
    "model = train_model(model, criteria, optimizer, dataloaders, exp_lr_scheduler, dataset_sizes, num_epochs=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DensenetUG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b1d4ebd50e0fd920647a78c7799c5f699a19636d20970a13f068d2efea0530da"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
